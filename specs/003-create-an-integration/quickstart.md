# Quickstart: Running Cross-Implementation Integration Tests

**Feature**: Cross-Implementation Integration Test Suite
**Audience**: Developers running tests locally
**Prerequisites**: Go 1.24+, Bun latest, both implementations built

---

## Prerequisites

Before running the integration test suite, ensure you have:

1. **Go 1.24 or later** installed
   ```bash
   go version  # Should show go1.24 or higher
   ```

2. **Bun latest** installed
   ```bash
   bun --version  # Should show latest stable version
   ```

3. **Both implementations built**:
   - Go implementation: `scitt` binary available in PATH or `./bin/scitt`
   - TypeScript implementation: Bun project with CLI accessible via `bun run cli`

4. **Dependencies installed**:
   ```bash
   # Install Go test dependencies
   cd tests/interop
   go mod download

   # Install TypeScript dependencies (if not already done)
   cd ../../  # back to repo root
   bun install
   ```

---

## Quick Start (30 seconds)

Run the complete test suite with default settings:

```bash
cd tests/interop
go test -v ./...
```

**Expected Output**:
```
=== RUN   TestCLICompatibility
=== RUN   TestCLICompatibility/init_command
=== RUN   TestCLICompatibility/statement_sign
...
--- PASS: TestCLICompatibility (2.34s)
...
PASS
ok      tests/interop   4.567s
```

---

## Running Specific Test Categories

### CLI Compatibility Tests

Test CLI command parity between Go and TypeScript implementations:

```bash
go test -v -run TestCLI ./cli
```

**What it tests**:
- `init` command compatibility
- `statement sign` command output equivalence
- `statement verify` command behavior
- `serve` command startup and configuration

### HTTP API Tests

Test HTTP endpoint compatibility:

```bash
go test -v -run TestHTTP ./http
```

**What it tests**:
- `POST /entries` registration
- `GET /entries/{entry_id}` retrieval
- `GET /checkpoint` consistency
- `GET /.well-known/transparency-configuration` metadata

### Cryptographic Interoperability Tests

Test cross-implementation signing and verification:

```bash
go test -v -run TestCrypto ./crypto
```

**What it tests**:
- Go signs â†’ TypeScript verifies (50+ combinations)
- TypeScript signs â†’ Go verifies (50+ combinations)
- JWK thumbprint consistency (RFC 7638)
- Hash envelope compatibility (RFC 6962)

### Merkle Tree Proof Tests

Test Merkle proof cross-validation:

```bash
go test -v -run TestMerkle ./merkle
```

**What it tests**:
- Inclusion proofs generated by Go, verified by TypeScript (30+ cases)
- Inclusion proofs generated by TypeScript, verified by Go (30+ cases)
- Consistency proofs for tree growth
- Root hash computation consistency

### End-to-End Workflow Tests

Test complete registration workflows with mixed implementations:

```bash
go test -v -run TestE2E ./e2e
```

**What it tests**:
- Go CLI + Go Server (baseline)
- TypeScript CLI + TypeScript Server (baseline)
- Go CLI + TypeScript Server (cross-implementation)
- TypeScript CLI + Go Server (cross-implementation)

---

## Running Tests in Parallel

Enable parallel test execution for faster results:

```bash
go test -v -parallel 10 ./...
```

**Note**: Tests use unique ports (20000-30000 range) to avoid conflicts.

---

## Running Specific Tests

### By Test Name

```bash
# Run only statement signing tests
go test -v -run TestStatementSign

# Run only error scenario tests
go test -v -run TestErrorScenarios
```

### By Tag (via environment variable)

```bash
# Run only negative tests
SCITT_TEST_TAGS=negative go test -v ./...

# Run only crypto tests
SCITT_TEST_TAGS=crypto go test -v ./...
```

---

## Test Output Formats

### Standard Output (default)

```bash
go test -v ./...
```

### JSON Output (for CI integration)

```bash
go test -json ./... > test-results.json
```

### Detailed Logs with Test Report

```bash
go test -v ./... 2>&1 | tee test-output.log

# Generate Markdown report from logs
go run ./lib/report.go < test-output.log > test-report.md
```

---

## Environment Configuration

### Custom Binary Paths

If implementations are not in standard locations:

```bash
# Specify Go CLI binary path
export SCITT_GO_CLI=/path/to/scitt

# Specify TypeScript CLI command
export SCITT_TS_CLI="bun run /path/to/cli/index.ts"

go test -v ./...
```

### Custom Port Range

To use a different port range (e.g., for restricted environments):

```bash
export SCITT_PORT_MIN=30000
export SCITT_PORT_MAX=40000

go test -v ./...
```

### Preserve Test Data on Failure

By default, temporary directories are cleaned up. To preserve for debugging:

```bash
export SCITT_CLEANUP_ON_FAIL=false

go test -v ./...
```

Failed test data will be preserved in `/tmp/scitt-test-*` directories with test name in path.

---

## Troubleshooting

### Tests Fail with "Port Already in Use"

**Cause**: Previous test run didn't clean up server processes.

**Solution**:
```bash
# Kill orphaned server processes
pkill -f "scitt serve"
pkill -f "bun run.*serve"

# Restart tests
go test -v ./...
```

### Tests Fail with "Binary Not Found"

**Cause**: Go or TypeScript CLI not accessible.

**Solution**:
```bash
# Build Go implementation
cd ../../  # back to repo root
go build -o bin/scitt ./cmd/scitt

# Verify TypeScript CLI works
bun run cli --help

# Set paths explicitly
export SCITT_GO_CLI=$(pwd)/bin/scitt
export SCITT_TS_CLI="bun run $(pwd)/cli/index.ts"

cd tests/interop
go test -v ./...
```

### Tests are Slow (>5 minutes)

**Cause**: Tests running sequentially or parallel flag not used.

**Solution**:
```bash
# Explicitly enable parallel execution
go test -v -parallel 10 ./...

# Check if tests are marked with t.Parallel()
grep -r "t.Parallel()" .
```

### JSON Comparison Failures

**Cause**: Implementations may use different JSON key ordering.

**Expected**: This is normal. Tests use semantic comparison, not string comparison.

**Check Report**: Look for `verdict` field in test output:
- `"identical"` = perfect match
- `"equivalent"` = semantically equal (different key order, acceptable numeric precision differences)
- `"divergent"` = semantic differences (test should fail)

---

## Understanding Test Results

### Test Status Meanings

- **PASS** âœ…: Both implementations behave identically or equivalently
- **FAIL** âŒ: Implementations diverge in behavior or output
- **SKIP** â­: Test skipped (e.g., feature not implemented yet)
- **ERROR** ðŸ”¥: Test infrastructure error (not implementation issue)

### Reading Comparison Output

When tests compare outputs, look for the `comparison` section:

```json
{
  "outputs_equivalent": true,
  "verdict": "equivalent",
  "verdict_reason": "JSON key ordering differs but content identical",
  "differences": [
    {
      "field_path": "$.entry_id",
      "severity": "acceptable",
      "explanation": "Both use hex encoding, different capitalization"
    }
  ]
}
```

**Severity Levels**:
- `critical`: Must fix (breaks RFC compliance)
- `major`: Should fix (breaks compatibility)
- `minor`: Nice to fix (cosmetic difference)
- `acceptable`: Won't fix (semantically equivalent)

---

## Generating Test Reports

### Markdown Summary Report

After running tests, generate a stakeholder-friendly report:

```bash
# Run tests with JSON output
go test -json ./... > results.json

# Generate Markdown report
go run ./lib/report.go --input results.json --output report.md

# View report
cat report.md
```

**Report includes**:
- Executive summary (pass/fail counts, duration)
- Failed test details with expected vs actual outputs
- RFC compliance violations with section references
- Performance metrics (slowest tests, parallelization ratio)

### CI Integration

For GitHub Actions, GitLab CI, or Jenkins:

```bash
# Run tests with JSON output for CI parsing
go test -json ./... > test-results.json

# Upload as artifact (GitHub Actions example)
# - uses: actions/upload-artifact@v3
#   with:
#     name: test-results
#     path: tests/interop/test-results.json
```

---

## Test Data Lifecycle

### Clean Slate Philosophy

Every test run starts with a clean slate:

1. **Before Each Test**:
   - Fresh temporary directories created (`t.TempDir()`)
   - Unique ports allocated (20000-30000 range)
   - Empty SQLite databases initialized
   - No state from previous tests

2. **After Each Test** (default behavior):
   - Temporary directories removed
   - Server processes stopped
   - Ports released
   - Databases deleted

### Preserving Test Data

To inspect test data after failure:

```bash
export SCITT_CLEANUP_ON_FAIL=false
go test -v -run TestSpecificTest ./...

# Find preserved data
ls -la /tmp/scitt-test-*
```

---

## Adding New Tests

### Test Structure

New tests should follow this pattern:

```go
func TestMyNewTest(t *testing.T) {
    t.Parallel() // Enable parallel execution

    // Setup isolated environment
    goDir, tsDir, cleanup := lib.SetupTestEnv(t)
    defer cleanup()

    // Allocate unique ports
    goPort := lib.AllocatePort(t)
    tsPort := lib.AllocatePort(t)

    // Run test logic
    goResult := lib.RunGoCLI([]string{"statement", "sign", "--input", "test.json"}, goDir)
    tsResult := lib.RunTsCLI([]string{"statement", "sign", "--input", "test.json"}, tsDir)

    // Compare results
    comparison := lib.CompareJSON(goResult.Stdout, tsResult.Stdout)
    if !comparison.OutputsEquivalent {
        t.Fatalf("Outputs diverged: %s", comparison.VerdictReason)
    }
}
```

### Test Fixtures

Add new fixtures in `tests/interop/fixtures/`:

```bash
# Generate new keypair fixture
go run ./tools/generate_keypair.go --output fixtures/keys/new_key.json

# Add custom payload fixture
echo '{"test": "data"}' > fixtures/payloads/new_payload.json

# Verify fixture schema compliance
go run ./tools/validate_fixture.go fixtures/keys/new_key.json
```

---

## FAQ

### Q: How long should tests take?

**A**: Complete test suite should finish in <5 minutes in CI, <3 minutes on modern local machines with `-parallel 10`.

### Q: Can I run tests without building both implementations?

**A**: No. Both Go binary and TypeScript CLI must be available. Tests validate cross-implementation behavior.

### Q: Do tests modify my local databases or storage?

**A**: No. Tests use isolated temporary directories (via `t.TempDir()`). Your production data is never touched.

### Q: What if one implementation is ahead of the other?

**A**: Tests may fail with `"feature not implemented"` errors. This is expected during development. Check test tags to skip known unimplemented features.

### Q: How do I debug a specific test failure?

**A**:
1. Preserve test data: `export SCITT_CLEANUP_ON_FAIL=false`
2. Run single test: `go test -v -run TestSpecificName`
3. Check test output for `expected vs actual` diffs
4. Inspect preserved data in `/tmp/scitt-test-*/`
5. Use `go test -race` to detect race conditions

---

## Next Steps

After running tests successfully:

1. **Review Test Report**: Check `test-report.md` for RFC compliance violations
2. **Fix Failures**: Address any `divergent` verdicts (not `equivalent`)
3. **Add New Tests**: Extend test coverage for new features
4. **CI Integration**: Add tests to CI pipeline (see `.github/workflows/` for examples)

---

## Support

For issues or questions:

- **Test Suite Issues**: File issue with `[integration-tests]` tag
- **Implementation Issues**: File issue with `[go]` or `[typescript]` tag
- **RFC Interpretation**: Reference `specs/003-create-an-integration/research.md` for decisions

---

## Summary

**Quick Commands**:
```bash
# Run all tests
go test -v ./...

# Run with parallelism (faster)
go test -v -parallel 10 ./...

# Run specific category
go test -v -run TestCLI ./cli

# Generate report
go test -json ./... > results.json
go run ./lib/report.go --input results.json --output report.md
```

**Key Principles**:
- Clean slate per test run (no persistent state)
- snake_case in all JSON interfaces
- hex encoding for identifiers
- RFC specifications as authoritative reference
- Go implementation as canonical source of truth (Principle VIII)

Happy testing! ðŸ§ª
